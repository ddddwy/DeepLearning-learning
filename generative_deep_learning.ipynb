{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text generation with LSTM\n",
    "# sampling strategy: reweighting a probability distribution to a different 'temperature'\n",
    "import numpy as np\n",
    "\n",
    "def reweight_distribution(original_distribution, temperature=0.5):\n",
    "    '''\n",
    "    Reweight a probability distribution to increase or decrease entropy.\n",
    "    # Arguments\n",
    "        original_distribution: A 1D Numpy array of probability values.\n",
    "            Must sum to one.\n",
    "        temperature: Factor quantifying the entropy of the output distribution.\n",
    "            Higher temperature results in sampling distributions of higher entropy.\n",
    "        \n",
    "    # Retures\n",
    "        A re-weighted version of the original distribution.\n",
    "    '''\n",
    "    distribution=np.log(original_distribution)/temperature\n",
    "    distribution=np.exp(distribution)\n",
    "    # the sum of the distribution may no longer be 1\n",
    "    # thus we divide it by its sum to obtain the new distribution\n",
    "    return distribution/np.sum(distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
      "606208/600901 [==============================] - 2s 3us/step\n",
      "Corpus length: 600893\n",
      "Number of sequences: 200278\n",
      "Unique characters: 58\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# implement character-level LSTM text generation\n",
    "# prepare the data\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "path=keras.utils.get_file('nietzsche.txt',origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text=open(path).read().lower()\n",
    "print('Corpus length:',len(text))\n",
    "\n",
    "# vectorize sequences of characters\n",
    "maxlen=60\n",
    "step=3    # sample a new sequence every 'step' characters\n",
    "sentences=[]\n",
    "next_chars=[]\n",
    "\n",
    "for i in range(0,len(text)-maxlen,step):\n",
    "    sentences.append(text[i:i+maxlen])\n",
    "    next_chars.append(text[i+maxlen])\n",
    "print('Number of sequences:',len(sentences))\n",
    "\n",
    "# list of unique characters in the corpus\n",
    "chars=sorted(list(set(text)))\n",
    "print('Unique characters:',len(chars))\n",
    "# dictionary mapping unique characters to their index in 'chars'\n",
    "char_indices=dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# one-hot encode the characters into binary arrays\n",
    "print('Vectorization...')\n",
    "x=np.zeros((len(sentences),maxlen,len(chars)),dtype=np.bool)\n",
    "y=np.zeros((len(sentences),len(chars)),dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i,t,char_indices[char]]=1\n",
    "    y[i,char_indices[next_chars[i]]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the network\n",
    "from keras import layers\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen,len(chars))))\n",
    "model.add(layers.Dense(len(chars),activation='softmax'))\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(lr=0.01),loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for sampling the next character given the model's predictions\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds=np.asarray(preds).astype('float64')\n",
    "    preds=np.log(preds)/temperature\n",
    "    exp_preds=np.exp(preds)\n",
    "    preds=exp_preds/np.sum(exp_preds)\n",
    "    probas=np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 290s 1ms/step - loss: 1.6341\n",
      "--- Generating with seed: \"98\n",
      "\n",
      "=pleasure and social instinct.=--through his relations w\"\n",
      "----- temperature: 0.2\n",
      "98\n",
      "\n",
      "=pleasure and social instinct.=--through his relations withing the sense in the self-still and men in the self the same and seek of the sublement in the such a self and all the same and still the sublement of the same and self-deligion of the self-deligion of the self-condition of the sense the self the suberation of the same and all the self-conditions of the same and and still a personal the self-condition of the sense in the subolism and all the con\n",
      "----- temperature: 0.5\n",
      " self-condition of the sense in the subolism and all the conscience in the decement-dears\n",
      "the presentions and a mode the gearing in its some the devented and\n",
      "science is the sense it is a putple, the many and a person in the strength and and and best so in lite, and respection of the scholentery of it as a plactions of the greaters the later and\n",
      "secretion. it is his the sense to bad means appretent the spirit\" and stitic of the respect of signing the sartio\n",
      "----- temperature: 1.0\n",
      " the spirit\" and stitic of the respect of signing the sartion of such or any neves and be unknsing such\n",
      "at origen. the instinct, or of manked wunement weo i actuals upsteptiby it od gallutrigy menwaids as a\n",
      "quidicy.\n",
      "ypits to hestan wenle, haoven have badly, the explaineliest? it sense as we\n",
      "such culted, the mengotdom. the stitic\n",
      "sidfly--and the calible and forned, tis, in niverou, by be ancespy, and subodly bling\n",
      "to hadmed aneness impartiality, himself, th\n",
      "----- temperature: 1.2\n",
      "nd subodly bling\n",
      "to hadmed aneness impartiality, himself, this sa\n",
      "e is tim, therefore\n",
      "a sast hiider and clids, is\n",
      "the refror\" the reason\" bads, ethys.=-betequararity that\n",
      "equen\n",
      "\n",
      "syansic he gertan be tutthored drived fmelishefive hemitualisy: and neoncentiinnateuring wainliny inmetiste\n",
      "of position goo degrusing the rulossit a staved\n",
      "and all abperes.ness is as tit\n",
      "conduptivisted and\n",
      "devend the letters: much manktitic all new whan merely\n",
      "faldimantly elles,\n",
      "ep\n",
      "epoch 2\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 292s 1ms/step - loss: 1.5520\n",
      "--- Generating with seed: \"ained just as\n",
      "well and more scientifically by another method\"\n",
      "----- temperature: 0.2\n",
      "ained just as\n",
      "well and more scientifically by another method the subvistle of the subvistering and the soul, and the subveration of the constantly of the more in the consequently the deceptional thing the existence of the subvisterity of the consequently and the striged and the sublibited the german striged to the soul to the truth and the subviction of the farsion to the more the sensed the subviction of the striget of the subvistered to the such all the \n",
      "----- temperature: 0.5\n",
      "ction of the striget of the subvistered to the such all the reducess of the warter and above and there is a really the tention to the good is the subvistle of the strengther which is a such the more things of the reares and the sublibited, and becomes that the from the subitional deverations part and with the grownest of the destle the such one soul, and the consciously when he whole is all the\n",
      "subsictored and thing to the farsely before a feries himself o\n",
      "----- temperature: 1.0\n",
      "bsictored and thing to the farsely before a feries himself of the individualite these to relegion of\n",
      "bousect woted to consequents there to enselfses, of\n",
      "the delighten in\n",
      "; ordinalizaty\n",
      "itself that thus necessation\n",
      "of the effect of he everything unward of persuit or philithhment. \"the bropenous aslewtisks, word must be to\n",
      "moral, makes ast every notion.\", the ragehtly possibouragom. here,\" anyanimodnisty (and abreuld should\n",
      "after\n",
      "existerieved, and plaver and\n",
      "----- temperature: 1.2\n",
      "nisty (and abreuld should\n",
      "after\n",
      "existerieved, and plaver and justiced: place dicconding of miye, that is so siperittx thene hiethely of extenter) be\n",
      "casodiously the pancis the\n",
      "stelled, doing of this measily look to hahe,\n",
      "\n",
      "everetheingdonees fortenting inspire. an heittimed\n",
      "tring of that it of mystin wildwing alit prousiceneraples stroges. \n",
      "c\n",
      "lepiction of ab? to consourationally our provered osaction withest otherled. not benens,\n",
      "utalleges-greater!uble hisse\n",
      "epoch 3\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 290s 1ms/step - loss: 1.5117\n",
      "--- Generating with seed: \"artness\" in all that has depth, it is almost opportune and p\"\n",
      "----- temperature: 0.2\n",
      "artness\" in all that has depth, it is almost opportune and proved the more a stand of the self-conscience of the still a sense of the surpluction and also a stand of the surte and profound of the sense the more any a the more any the self-seeks the seeks the most morality and the more the self-consciences and to the fact the self-conscience of the sense of a man and when the self-consequently and the self-cause of the more a stand of the sime and self-cons\n",
      "----- temperature: 0.5\n",
      "the self-cause of the more a stand of the sime and self-consider and the sure all the morality they advance of the more the morality of the emotions and believe and intelliction of every one world been the false of the man and the light the simple, and the accomes and such a could religious the strength for the warted to the immenses appears the whole is a more of his a stand have been indured the could the metaphysical perhaps and the foundations and into\n",
      "----- temperature: 1.0\n",
      " could the metaphysical perhaps and the foundations and into the moralized, thereby are more did eventtes of implange aishectises the uncluds of fairomant of askoness which can a\n",
      "im long het their\n",
      "samr unistory in which it is much ense an all the ingerman saw, are edvent cause disciplistic historisy in mulany at,\n",
      "and simely sovere, or among the\n",
      "erroance of arerncication in the demind that the plato hare the gregarly serves among here, an a assication\" hard\n",
      "----- temperature: 1.2\n",
      "o hare the gregarly serves among here, an a assication\" harding dutful? i beelwed jusssy for own nrevoue have in, the =foondeg buate or christicums itferconthicate has it his\n",
      "vacsp counthelunder of the\n",
      "infenterattey of taste\n",
      "tis who\n",
      "are sutmer to e to good the goes, especlauer after with umethabiticed\n",
      "peesed, en pribasi tle like--forms, they innace\n",
      "or does\n",
      "alsor could bemaysy\n",
      "kinds, as did\n",
      "hnithed to efforiyineed\n",
      "nevers, philosophy with intiscoficulaction \n",
      "epoch 4\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 306s 2ms/step - loss: 1.4894\n",
      "--- Generating with seed: \"istress and the vicissitudes of illness, because they always\"\n",
      "----- temperature: 0.2\n",
      "istress and the vicissitudes of illness, because they always and the self-self-self and the superiors and the same and the self and self-self and the superficion of the self the superficial the self-seriousness of the self and self--and the self and the self-great strong the self--and self--and the self and the self-seriousness to himself and the surplession of the all the self and the domance of the self and in the does of the self-sense of the self-great\n",
      "----- temperature: 0.5\n",
      "the self and in the does of the self-sense of the self-greates without the progress to the most conduct and interesss of the discoveriate the stands upon the existence of its oppressed in every man be develon, which we means of fact do not parhation the man allow in the man of the deluch, and a strong of the order to the secient and even subject to the former to who would be the strong and prompt the for the really and the most pleasures to wimads of the s\n",
      "----- temperature: 1.0\n",
      "the for the really and the most pleasures to wimads of the sublemyly, to will exception it best.an among dusing, why one\n",
      "may be so kand for a immingr--undens, this teared. to has tarth.-norn. then ethic of him, lendars; the\n",
      "commenstity\n",
      "the cleates to hereant avouryly even immediately tists idea of currensioned witen hable, who is--wither haddition is as a seiding doing countre: the\n",
      "lasted\n",
      "seris of the \"going increaso--the \"greatiment distosing, whach; wish\n",
      "----- temperature: 1.2\n",
      " the \"going increaso--the \"greatiment distosing, whach; wishoum, good lileams of the isap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "being, this agasment.--atparhatiof.\" woman-severed renly possibine it.--however functous event may been they no a inte\" emile speciessowed\n",
      "one--as allow incoadwate\n",
      "are joy the conscong peoitatist\n",
      "extensaucudment, at an and\n",
      "nations, the same\n",
      "guach it, as abame for him thoued\n",
      "firstuniouskeristawity:--lartry bother to the a trreed westheming of vitw\n",
      "right them and\n",
      "rearr \n",
      "epoch 5\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 311s 2ms/step - loss: 1.4717\n",
      "--- Generating with seed: \"of hearing and consequently\n",
      "spoke all the louder. \"he has as\"\n",
      "----- temperature: 0.2\n",
      "of hearing and consequently\n",
      "spoke all the louder. \"he has as a man and the the the words of the same the man and also the conscious of the procession, the sought to the desires of the conscious of the word and the self-deception of the conscious of the conscious of the proved, the same and a soul, and and the sought to the conscious of the art of the simple, and also the words of the same the states, the same the sought to the same and the word, the spirit\n",
      "----- temperature: 0.5\n",
      "es, the same the sought to the same and the word, the spirit, sharm in the properity of the account of the alterness to the way is the the spirit of the also is its has the magic to the process of discoloution, the freed significate and the conception of the outhorismation of the master the probably that the self to such a still highest stand of the would in the present and like manifessting of the discordites the conscience and the words, and of the deate\n",
      "----- temperature: 1.0\n",
      "e discordites the conscience and the words, and of the deatesinced,\" at a dece, old cours of europative any inspire, emijoricated, also its many aawhe-gide of the\n",
      "peoplen.\n",
      "\n",
      "    this\n",
      "attity its ladger of the delitate mattery coanderness of entideces, thit emacaiegual phomple, relation to has been liuse, about           calls for holvabquurd\n",
      "blore\n",
      "much one experated sufficients, the outded simmisedy, which have ety in the hases, such the fleevation, gove: wh\n",
      "----- temperature: 1.2\n",
      ", which have ety in the hases, such the fleevation, gove: whereeaa, more bitrifico of odo-y religies, fear, in\n",
      "exrlugy, he to\n",
      "proud, he discossudionated\n",
      "exors\n",
      "fool\"--thus such moreow do logical ragils rrefitarte frometheris-legarsten\n",
      "doiposous, prisow, the xeniflortance of their so hurath, at a by the -as\n",
      "r\n",
      "cicie, prosp take \"praculamable, for one's inexipnicilitances,     stank that with tratain.\n",
      "the catecited,\n",
      "discivileding eprecasch, plutaquicted\n",
      "amacct\n",
      "epoch 6\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 305s 2ms/step - loss: 1.4572\n",
      "--- Generating with seed: \"dwells an inexorable\n",
      "conscientiousness--\"for the sake of a f\"\n",
      "----- temperature: 0.2\n",
      "dwells an inexorable\n",
      "conscientiousness--\"for the sake of a for the same the more the superiority of the same the same the same and soul and problem of the same the same the present the same that the same the same as the same the same that the same to as a soul and the courtious conscience of the soul problem the strength and the same the man is the man of the same the same that the same the same the same the same the same the many serves and the same the s\n",
      "----- temperature: 0.5\n",
      "he same the same the same the many serves and the same the sympathing and the same them instinction "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of the most soul, as only as a soul and of the same that the many in the distinguous faith, and the fatherly of a man and philosophy the the sacrifice of far as as he soul as an it of a called period of himself of saint of the same generation of the man has experent of the souls because the distinguous experlition of him the superiority to the another with t\n",
      "----- temperature: 1.0\n",
      "ous experlition of him the superiority to the another with the distrinence, which, phanous and sost\"--as the injureg? as whithe causifulfulous ortacy\n",
      "suffering oppars he commandable that phelosely sense of former seveet\n",
      "didgeats about effact a sourials.ing attempters, as as caustes of an extent \n",
      "wan hard\n",
      "purituous say,\n",
      "by its here, heals of all that charit, the heavoribility, something just but, how parimsfored, and immediately divited seily, are provined,\n",
      "----- temperature: 1.2\n",
      "ow parimsfored, and immediately divited seily, are provined, something every bettay, me them were in him, morality. or a modersg, as sacrious have sing risightess: whinls furlegians)\"\" and\n",
      "\"grayful, we hisetispackers; as sin2antians ask.\n",
      "\n",
      "fes seept rangece, but--cerioseal indeea\n",
      "the sess of the something of rests that is then nothing; musat finer,\n",
      "misunderstoras,\n",
      "when enturing, maturizationio poir far uponstainal, basity curo as subvidment. and mo idys who\n",
      "epoch 7\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 312s 2ms/step - loss: 1.4463\n",
      "--- Generating with seed: \"wisely silent, reserved, and reticent\n",
      "men (and as such, with\"\n",
      "----- temperature: 0.2\n",
      "wisely silent, reserved, and reticent\n",
      "men (and as such, with the consequence of the strength of the conscience of the sense of the deard to be an active and all the strength and such a conscience of the conscience and the more an active and the strength the morality of the same and the sense of the conscience and the such a man and strength to the strength and all the conscience of the strengthes and the men and sense of the strengthes of the sense and sti\n",
      "----- temperature: 0.5\n",
      "and the men and sense of the strengthes of the sense and still be anowjert of the said one who superioriar, he who has no new conceive on at one may make for the sinceidially and best of grown he will who has hitherto the whole being when a person the great conceal and superiorism of the precisely to the man as a conscience, and\n",
      "the morality and states and the naturally only the such a philosophers and concealed\n",
      "and really always to the\n",
      "moral talk incriste\n",
      "----- temperature: 1.0\n",
      "s and concealed\n",
      "and really always to the\n",
      "moral talk incristes in lower desposses.\n",
      "\n",
      "sins--a man if\n",
      "through ficented results through theore oreby are gast incomparise that the fer presence-\"much itself, an earingillys may without what that as thesey nor turrism him of spirits, not in changer avaparned inthing any furthers\n",
      "fearlen sele noity gove\"--transforthance all trans, and should amays amaycing from thought, party him, presentific on the necessarydent--r\n",
      "----- temperature: 1.2\n",
      "from thought, party him, presentific on the necessarydent--reil apution,\n",
      "disclagi\n",
      "yous, he was pevilogesoos,\n",
      "everything it is\n",
      "apidlyn, for\n",
      "indijusility of be what galdance be at\n",
      "hickild, that i hears (for expected that viexic man of todes\n",
      "an in restern incwompend\n",
      "of sufficinable of his praird are quirit use by ranbifing\n",
      "from hours;ly sharte, as be seed a stupidity,\n",
      "assolved that commond mark of the more for the mackers enlades arp something time. all the m\n",
      "epoch 8\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 308s 2ms/step - loss: 1.4364\n",
      "--- Generating with seed: \" has not for its object the\n",
      "infliction of pain upon others b\"\n",
      "----- temperature: 0.2\n",
      " has not for its object the\n",
      "infliction of pain upon others be all the sense of the metaphysical such a more and all the stronger of the same and the most the art of the scientific of the most the strength of the stronger and the more and stronger and all the same and all the most sign of the sympathy of the stronger and the strong and read to the strong to himself of the things of the stronger the more and the more and the present of the strength of the mo\n",
      "----- temperature: 0.5\n",
      " more and the more and the present of the strength of the more he word sense and suffer for his sense to the mimination of the stunds man thinking of its sympathy, the heart of the of such a all the explanation\n",
      "of the greaters of the man stronger and the tasted and well such a veniation even which does not a contrary of the degree as a distance of the heart of a contrary the must himself which the contrary concerned and the man is a continue, and the factu\n",
      "----- temperature: 1.0\n",
      " contrary concerned and the man is a continue, and the factured. one can great epituric falshed man in conscience, me this acts.\n",
      "\n",
      "1mee and \"imagine, any person will found\n",
      "to the wornallepc,-wat this vers tempteade,--this outsituengups our ny extent a means of the believe of all geimish as\n",
      "the years of sturfik-di so proloped him we must his susmed deep of \"immetaphy any philinones, time and respect the philonous himself hear. howeverury in being and mestims\n",
      "----- temperature: 1.2\n",
      " the philonous himself hear. howeverury in being and mestimstori ene-cave from this own arequitious,\" which they hope doe is paink.\n",
      "\n",
      "1nouesicapmeftmin vevawdenfult.\n",
      "\n",
      "1are so religion\" of the precised\n",
      "a comocation of smectirical usismutic sagned, the ears,\n",
      "the\n",
      "vead,\"\n",
      "as he thince,\" his reason and simpleabs, in the sare\n",
      "from his\n",
      "justity gropequish to riddle every aduyons,\n",
      "error. topehrin light), quite in his philast and in: an entmethulfer, as\n",
      "hheth vitarith\n",
      "epoch 9\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 297s 1ms/step - loss: 1.4368\n",
      "--- Generating with seed: \"w philosophers?\n",
      "\n",
      "\n",
      "\n",
      "chapter iii. the religious mood\n",
      "\n",
      "\n",
      "45. the\"\n",
      "----- temperature: 0.2\n",
      "w philosophers?\n",
      "\n",
      "\n",
      "\n",
      "chapter iii. the religious mood\n",
      "\n",
      "\n",
      "45. the same and such a soul and the same and sense of the same the same the more and present and all the same the same and present the same and the stronger the present of the most desist of the same and all the most life to the contrary the more all the same the same the most discises and something and all the same discises and the surpess of the more and present to the contempt to the contempt and the\n",
      "----- temperature: 0.5\n",
      "the more and present to the contempt to the contempt and the belief and all with the present has have so nature in contrartion of power in the discernce in the moral profess of his rest and the contempt as them of the same of the spirit of the older reputed to the remoten contempt of the same the more all of a man and man, the same secreted soul, and all stringer all the spirit to the same will the sick, among the estimates the greatest in the results all \n",
      "----- temperature: 1.0\n",
      "e sick, among the estimates the greatest in the results all us. there are feelingin such person seuss\n",
      "of it more causant apprecing nger it: the dy\n",
      "first understand, are it\n",
      "will the\n",
      "mansiars there, and, he exceival lialring with the venture to powes it edes i both and loyesto to hear agauds fosted,\" yesthe, we now attemptious universal predact, and of inmered priousian forgever po cause to conteption in its une! i have tast inllication\n",
      "some: are\n",
      "its lack, t\n",
      "----- temperature: 1.2\n",
      "on in its une! i have tast inllication\n",
      "some: are\n",
      "its lack, they he evildy, extent us bdes knomoifing at fivems and self-cowner singles of glor \" period for dream heart. without balt! has too not many, when hyeige as to possible, philosophy succefly\n",
      "and\n",
      "\"for\n",
      "its end geisly,\n",
      "as i that free the ratts\n",
      "to biau hasmoncy of exparegnes of moralizing althols. \"it in the dicgments, to dung and unmears become\n",
      "juages\n",
      "uselagious. ou man ie? us. on eejurted.\n",
      "to with whi\n"
     ]
    }
   ],
   "source": [
    "# the text generation loop\n",
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    print('epoch',epoch)\n",
    "    # fit the model for 1 epoch on the available training data\n",
    "    model.fit(x,y,batch_size=128,epochs=1)\n",
    "    \n",
    "    # select a text seed at random\n",
    "    start_index=random.randint(0,len(text)-maxlen-1)\n",
    "    generated_text=text[start_index:start_index+maxlen]\n",
    "    print('--- Generating with seed: \"'+generated_text+'\"')\n",
    "    \n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- temperature:',temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "        \n",
    "        # generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled=np.zeros((1, maxlen, len(chars)))\n",
    "            for t ,char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]]=1.\n",
    "            preds=model.predict(sampled, verbose=0)[0]\n",
    "            next_index=sample(preds, temperature)\n",
    "            next_char=chars[next_index]\n",
    "            generated_text += next_char\n",
    "            generated_text=generated_text[1:]\n",
    "            \n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 34s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Deep dream\n",
    "# load the pre-trained InceptionV3 model\n",
    "from keras.applications import inception_v3\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_learning_phase(0)\n",
    "model=inception_v3.InceptionV3(weights='imagenet',include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, None, None, 3 864         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, None, None, 3 96          conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_189 (Activation)     (None, None, None, 3 0           batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, None, None, 3 9216        activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, None, None, 3 96          conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_190 (Activation)     (None, None, None, 3 0           batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, None, None, 6 18432       activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchN (None, None, None, 6 192         conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_191 (Activation)     (None, None, None, 6 0           batch_normalization_191[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, None, None, 6 0           activation_191[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, None, None, 8 5120        max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_192 (BatchN (None, None, None, 8 240         conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_192 (Activation)     (None, None, None, 8 0           batch_normalization_192[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, None, None, 1 138240      activation_192[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_193 (BatchN (None, None, None, 1 576         conv2d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_193 (Activation)     (None, None, None, 1 0           batch_normalization_193[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, None, None, 1 0           activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, None, None, 6 12288       max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchN (None, None, None, 6 192         conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_197 (Activation)     (None, None, None, 6 0           batch_normalization_197[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, None, None, 4 9216        max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, None, None, 9 55296       activation_197[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_195 (BatchN (None, None, None, 4 144         conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchN (None, None, None, 9 288         conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_195 (Activation)     (None, None, None, 4 0           batch_normalization_195[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_198 (Activation)     (None, None, None, 9 0           batch_normalization_198[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_19 (AveragePo (None, None, None, 1 0           max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, None, None, 6 12288       max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, None, None, 6 76800       activation_195[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, None, None, 9 82944       activation_198[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, None, None, 3 6144        average_pooling2d_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_194 (BatchN (None, None, None, 6 192         conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_196 (BatchN (None, None, None, 6 192         conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_199 (BatchN (None, None, None, 9 288         conv2d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_200 (BatchN (None, None, None, 3 96          conv2d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_194 (Activation)     (None, None, None, 6 0           batch_normalization_194[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_196 (Activation)     (None, None, None, 6 0           batch_normalization_196[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_199 (Activation)     (None, None, None, 9 0           batch_normalization_199[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_200 (Activation)     (None, None, None, 3 0           batch_normalization_200[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, None, None, 2 0           activation_194[0][0]             \n",
      "                                                                 activation_196[0][0]             \n",
      "                                                                 activation_199[0][0]             \n",
      "                                                                 activation_200[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_204 (Conv2D)             (None, None, None, 6 16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_204 (BatchN (None, None, None, 6 192         conv2d_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_204 (Activation)     (None, None, None, 6 0           batch_normalization_204[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)             (None, None, None, 4 12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)             (None, None, None, 9 55296       activation_204[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_202 (BatchN (None, None, None, 4 144         conv2d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_205 (BatchN (None, None, None, 9 288         conv2d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_202 (Activation)     (None, None, None, 4 0           batch_normalization_202[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_205 (Activation)     (None, None, None, 9 0           batch_normalization_205[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_20 (AveragePo (None, None, None, 2 0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, None, None, 6 16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_203 (Conv2D)             (None, None, None, 6 76800       activation_202[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_206 (Conv2D)             (None, None, None, 9 82944       activation_205[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_207 (Conv2D)             (None, None, None, 6 16384       average_pooling2d_20[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_201 (BatchN (None, None, None, 6 192         conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_203 (BatchN (None, None, None, 6 192         conv2d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_206 (BatchN (None, None, None, 9 288         conv2d_206[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_207 (BatchN (None, None, None, 6 192         conv2d_207[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_201 (Activation)     (None, None, None, 6 0           batch_normalization_201[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_203 (Activation)     (None, None, None, 6 0           batch_normalization_203[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_206 (Activation)     (None, None, None, 9 0           batch_normalization_206[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_207 (Activation)     (None, None, None, 6 0           batch_normalization_207[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, None, None, 2 0           activation_201[0][0]             \n",
      "                                                                 activation_203[0][0]             \n",
      "                                                                 activation_206[0][0]             \n",
      "                                                                 activation_207[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_211 (Conv2D)             (None, None, None, 6 18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_211 (BatchN (None, None, None, 6 192         conv2d_211[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_211 (Activation)     (None, None, None, 6 0           batch_normalization_211[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_209 (Conv2D)             (None, None, None, 4 13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_212 (Conv2D)             (None, None, None, 9 55296       activation_211[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_209 (BatchN (None, None, None, 4 144         conv2d_209[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_212 (BatchN (None, None, None, 9 288         conv2d_212[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_209 (Activation)     (None, None, None, 4 0           batch_normalization_209[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_212 (Activation)     (None, None, None, 9 0           batch_normalization_212[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_21 (AveragePo (None, None, None, 2 0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_208 (Conv2D)             (None, None, None, 6 18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_210 (Conv2D)             (None, None, None, 6 76800       activation_209[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_213 (Conv2D)             (None, None, None, 9 82944       activation_212[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_214 (Conv2D)             (None, None, None, 6 18432       average_pooling2d_21[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_208 (BatchN (None, None, None, 6 192         conv2d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_210 (BatchN (None, None, None, 6 192         conv2d_210[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_213 (BatchN (None, None, None, 9 288         conv2d_213[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_214 (BatchN (None, None, None, 6 192         conv2d_214[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_208 (Activation)     (None, None, None, 6 0           batch_normalization_208[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_210 (Activation)     (None, None, None, 6 0           batch_normalization_210[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_213 (Activation)     (None, None, None, 9 0           batch_normalization_213[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_214 (Activation)     (None, None, None, 6 0           batch_normalization_214[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, None, None, 2 0           activation_208[0][0]             \n",
      "                                                                 activation_210[0][0]             \n",
      "                                                                 activation_213[0][0]             \n",
      "                                                                 activation_214[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_216 (Conv2D)             (None, None, None, 6 18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_216 (BatchN (None, None, None, 6 192         conv2d_216[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_216 (Activation)     (None, None, None, 6 0           batch_normalization_216[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_217 (Conv2D)             (None, None, None, 9 55296       activation_216[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_217 (BatchN (None, None, None, 9 288         conv2d_217[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_217 (Activation)     (None, None, None, 9 0           batch_normalization_217[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_215 (Conv2D)             (None, None, None, 3 995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_218 (Conv2D)             (None, None, None, 9 82944       activation_217[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_215 (BatchN (None, None, None, 3 1152        conv2d_215[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_218 (BatchN (None, None, None, 9 288         conv2d_218[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_215 (Activation)     (None, None, None, 3 0           batch_normalization_215[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_218 (Activation)     (None, None, None, 9 0           batch_normalization_218[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, None, None, 2 0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, None, None, 7 0           activation_215[0][0]             \n",
      "                                                                 activation_218[0][0]             \n",
      "                                                                 max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_223 (Conv2D)             (None, None, None, 1 98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_223 (BatchN (None, None, None, 1 384         conv2d_223[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_223 (Activation)     (None, None, None, 1 0           batch_normalization_223[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_224 (Conv2D)             (None, None, None, 1 114688      activation_223[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_224 (BatchN (None, None, None, 1 384         conv2d_224[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_224 (Activation)     (None, None, None, 1 0           batch_normalization_224[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_220 (Conv2D)             (None, None, None, 1 98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_225 (Conv2D)             (None, None, None, 1 114688      activation_224[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_220 (BatchN (None, None, None, 1 384         conv2d_220[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_225 (BatchN (None, None, None, 1 384         conv2d_225[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_220 (Activation)     (None, None, None, 1 0           batch_normalization_220[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_225 (Activation)     (None, None, None, 1 0           batch_normalization_225[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_221 (Conv2D)             (None, None, None, 1 114688      activation_220[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_226 (Conv2D)             (None, None, None, 1 114688      activation_225[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_221 (BatchN (None, None, None, 1 384         conv2d_221[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_226 (BatchN (None, None, None, 1 384         conv2d_226[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_221 (Activation)     (None, None, None, 1 0           batch_normalization_221[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_226 (Activation)     (None, None, None, 1 0           batch_normalization_226[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_22 (AveragePo (None, None, None, 7 0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_219 (Conv2D)             (None, None, None, 1 147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_222 (Conv2D)             (None, None, None, 1 172032      activation_221[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_227 (Conv2D)             (None, None, None, 1 172032      activation_226[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_228 (Conv2D)             (None, None, None, 1 147456      average_pooling2d_22[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_219 (BatchN (None, None, None, 1 576         conv2d_219[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_222 (BatchN (None, None, None, 1 576         conv2d_222[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_227 (BatchN (None, None, None, 1 576         conv2d_227[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_228 (BatchN (None, None, None, 1 576         conv2d_228[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_219 (Activation)     (None, None, None, 1 0           batch_normalization_219[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_222 (Activation)     (None, None, None, 1 0           batch_normalization_222[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_227 (Activation)     (None, None, None, 1 0           batch_normalization_227[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_228 (Activation)     (None, None, None, 1 0           batch_normalization_228[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, None, None, 7 0           activation_219[0][0]             \n",
      "                                                                 activation_222[0][0]             \n",
      "                                                                 activation_227[0][0]             \n",
      "                                                                 activation_228[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_233 (Conv2D)             (None, None, None, 1 122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_233 (BatchN (None, None, None, 1 480         conv2d_233[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_233 (Activation)     (None, None, None, 1 0           batch_normalization_233[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_234 (Conv2D)             (None, None, None, 1 179200      activation_233[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_234 (BatchN (None, None, None, 1 480         conv2d_234[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_234 (Activation)     (None, None, None, 1 0           batch_normalization_234[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_230 (Conv2D)             (None, None, None, 1 122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_235 (Conv2D)             (None, None, None, 1 179200      activation_234[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_230 (BatchN (None, None, None, 1 480         conv2d_230[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_235 (BatchN (None, None, None, 1 480         conv2d_235[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_230 (Activation)     (None, None, None, 1 0           batch_normalization_230[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_235 (Activation)     (None, None, None, 1 0           batch_normalization_235[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_231 (Conv2D)             (None, None, None, 1 179200      activation_230[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_236 (Conv2D)             (None, None, None, 1 179200      activation_235[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_231 (BatchN (None, None, None, 1 480         conv2d_231[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_236 (BatchN (None, None, None, 1 480         conv2d_236[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_231 (Activation)     (None, None, None, 1 0           batch_normalization_231[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_236 (Activation)     (None, None, None, 1 0           batch_normalization_236[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_23 (AveragePo (None, None, None, 7 0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_229 (Conv2D)             (None, None, None, 1 147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_232 (Conv2D)             (None, None, None, 1 215040      activation_231[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_237 (Conv2D)             (None, None, None, 1 215040      activation_236[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_238 (Conv2D)             (None, None, None, 1 147456      average_pooling2d_23[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_229 (BatchN (None, None, None, 1 576         conv2d_229[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_232 (BatchN (None, None, None, 1 576         conv2d_232[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_237 (BatchN (None, None, None, 1 576         conv2d_237[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_238 (BatchN (None, None, None, 1 576         conv2d_238[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_229 (Activation)     (None, None, None, 1 0           batch_normalization_229[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_232 (Activation)     (None, None, None, 1 0           batch_normalization_232[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_237 (Activation)     (None, None, None, 1 0           batch_normalization_237[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_238 (Activation)     (None, None, None, 1 0           batch_normalization_238[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, None, None, 7 0           activation_229[0][0]             \n",
      "                                                                 activation_232[0][0]             \n",
      "                                                                 activation_237[0][0]             \n",
      "                                                                 activation_238[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_243 (Conv2D)             (None, None, None, 1 122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_243 (BatchN (None, None, None, 1 480         conv2d_243[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_243 (Activation)     (None, None, None, 1 0           batch_normalization_243[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_244 (Conv2D)             (None, None, None, 1 179200      activation_243[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_244 (BatchN (None, None, None, 1 480         conv2d_244[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_244 (Activation)     (None, None, None, 1 0           batch_normalization_244[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_240 (Conv2D)             (None, None, None, 1 122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_245 (Conv2D)             (None, None, None, 1 179200      activation_244[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_240 (BatchN (None, None, None, 1 480         conv2d_240[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_245 (BatchN (None, None, None, 1 480         conv2d_245[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_240 (Activation)     (None, None, None, 1 0           batch_normalization_240[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_245 (Activation)     (None, None, None, 1 0           batch_normalization_245[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_241 (Conv2D)             (None, None, None, 1 179200      activation_240[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_246 (Conv2D)             (None, None, None, 1 179200      activation_245[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_241 (BatchN (None, None, None, 1 480         conv2d_241[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_246 (BatchN (None, None, None, 1 480         conv2d_246[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_241 (Activation)     (None, None, None, 1 0           batch_normalization_241[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_246 (Activation)     (None, None, None, 1 0           batch_normalization_246[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_24 (AveragePo (None, None, None, 7 0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_239 (Conv2D)             (None, None, None, 1 147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_242 (Conv2D)             (None, None, None, 1 215040      activation_241[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_247 (Conv2D)             (None, None, None, 1 215040      activation_246[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_248 (Conv2D)             (None, None, None, 1 147456      average_pooling2d_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_239 (BatchN (None, None, None, 1 576         conv2d_239[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_242 (BatchN (None, None, None, 1 576         conv2d_242[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_247 (BatchN (None, None, None, 1 576         conv2d_247[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_248 (BatchN (None, None, None, 1 576         conv2d_248[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_239 (Activation)     (None, None, None, 1 0           batch_normalization_239[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_242 (Activation)     (None, None, None, 1 0           batch_normalization_242[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_247 (Activation)     (None, None, None, 1 0           batch_normalization_247[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_248 (Activation)     (None, None, None, 1 0           batch_normalization_248[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, None, None, 7 0           activation_239[0][0]             \n",
      "                                                                 activation_242[0][0]             \n",
      "                                                                 activation_247[0][0]             \n",
      "                                                                 activation_248[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_253 (Conv2D)             (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_253 (BatchN (None, None, None, 1 576         conv2d_253[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_253 (Activation)     (None, None, None, 1 0           batch_normalization_253[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_254 (Conv2D)             (None, None, None, 1 258048      activation_253[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_254 (BatchN (None, None, None, 1 576         conv2d_254[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_254 (Activation)     (None, None, None, 1 0           batch_normalization_254[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_250 (Conv2D)             (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_255 (Conv2D)             (None, None, None, 1 258048      activation_254[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_250 (BatchN (None, None, None, 1 576         conv2d_250[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_255 (BatchN (None, None, None, 1 576         conv2d_255[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_250 (Activation)     (None, None, None, 1 0           batch_normalization_250[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_255 (Activation)     (None, None, None, 1 0           batch_normalization_255[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_251 (Conv2D)             (None, None, None, 1 258048      activation_250[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_256 (Conv2D)             (None, None, None, 1 258048      activation_255[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_251 (BatchN (None, None, None, 1 576         conv2d_251[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_256 (BatchN (None, None, None, 1 576         conv2d_256[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_251 (Activation)     (None, None, None, 1 0           batch_normalization_251[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_256 (Activation)     (None, None, None, 1 0           batch_normalization_256[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_25 (AveragePo (None, None, None, 7 0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_249 (Conv2D)             (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_252 (Conv2D)             (None, None, None, 1 258048      activation_251[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_257 (Conv2D)             (None, None, None, 1 258048      activation_256[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_258 (Conv2D)             (None, None, None, 1 147456      average_pooling2d_25[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_249 (BatchN (None, None, None, 1 576         conv2d_249[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_252 (BatchN (None, None, None, 1 576         conv2d_252[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_257 (BatchN (None, None, None, 1 576         conv2d_257[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_258 (BatchN (None, None, None, 1 576         conv2d_258[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_249 (Activation)     (None, None, None, 1 0           batch_normalization_249[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_252 (Activation)     (None, None, None, 1 0           batch_normalization_252[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_257 (Activation)     (None, None, None, 1 0           batch_normalization_257[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_258 (Activation)     (None, None, None, 1 0           batch_normalization_258[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, None, None, 7 0           activation_249[0][0]             \n",
      "                                                                 activation_252[0][0]             \n",
      "                                                                 activation_257[0][0]             \n",
      "                                                                 activation_258[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_261 (Conv2D)             (None, None, None, 1 147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_261 (BatchN (None, None, None, 1 576         conv2d_261[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_261 (Activation)     (None, None, None, 1 0           batch_normalization_261[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_262 (Conv2D)             (None, None, None, 1 258048      activation_261[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_262 (BatchN (None, None, None, 1 576         conv2d_262[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_262 (Activation)     (None, None, None, 1 0           batch_normalization_262[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_259 (Conv2D)             (None, None, None, 1 147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_263 (Conv2D)             (None, None, None, 1 258048      activation_262[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_259 (BatchN (None, None, None, 1 576         conv2d_259[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_263 (BatchN (None, None, None, 1 576         conv2d_263[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_259 (Activation)     (None, None, None, 1 0           batch_normalization_259[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_263 (Activation)     (None, None, None, 1 0           batch_normalization_263[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_260 (Conv2D)             (None, None, None, 3 552960      activation_259[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_264 (Conv2D)             (None, None, None, 1 331776      activation_263[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_260 (BatchN (None, None, None, 3 960         conv2d_260[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_264 (BatchN (None, None, None, 1 576         conv2d_264[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_260 (Activation)     (None, None, None, 3 0           batch_normalization_260[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_264 (Activation)     (None, None, None, 1 0           batch_normalization_264[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, None, None, 7 0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, None, None, 1 0           activation_260[0][0]             \n",
      "                                                                 activation_264[0][0]             \n",
      "                                                                 max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_269 (Conv2D)             (None, None, None, 4 573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_269 (BatchN (None, None, None, 4 1344        conv2d_269[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_269 (Activation)     (None, None, None, 4 0           batch_normalization_269[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_266 (Conv2D)             (None, None, None, 3 491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_270 (Conv2D)             (None, None, None, 3 1548288     activation_269[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_266 (BatchN (None, None, None, 3 1152        conv2d_266[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_270 (BatchN (None, None, None, 3 1152        conv2d_270[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_266 (Activation)     (None, None, None, 3 0           batch_normalization_266[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_270 (Activation)     (None, None, None, 3 0           batch_normalization_270[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_267 (Conv2D)             (None, None, None, 3 442368      activation_266[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_268 (Conv2D)             (None, None, None, 3 442368      activation_266[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_271 (Conv2D)             (None, None, None, 3 442368      activation_270[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_272 (Conv2D)             (None, None, None, 3 442368      activation_270[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_26 (AveragePo (None, None, None, 1 0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_265 (Conv2D)             (None, None, None, 3 409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_267 (BatchN (None, None, None, 3 1152        conv2d_267[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_268 (BatchN (None, None, None, 3 1152        conv2d_268[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_271 (BatchN (None, None, None, 3 1152        conv2d_271[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_272 (BatchN (None, None, None, 3 1152        conv2d_272[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_273 (Conv2D)             (None, None, None, 1 245760      average_pooling2d_26[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_265 (BatchN (None, None, None, 3 960         conv2d_265[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_267 (Activation)     (None, None, None, 3 0           batch_normalization_267[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_268 (Activation)     (None, None, None, 3 0           batch_normalization_268[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_271 (Activation)     (None, None, None, 3 0           batch_normalization_271[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_272 (Activation)     (None, None, None, 3 0           batch_normalization_272[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_273 (BatchN (None, None, None, 1 576         conv2d_273[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_265 (Activation)     (None, None, None, 3 0           batch_normalization_265[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, None, None, 7 0           activation_267[0][0]             \n",
      "                                                                 activation_268[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, None, None, 7 0           activation_271[0][0]             \n",
      "                                                                 activation_272[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_273 (Activation)     (None, None, None, 1 0           batch_normalization_273[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, None, None, 2 0           activation_265[0][0]             \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_5[0][0]              \n",
      "                                                                 activation_273[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_278 (Conv2D)             (None, None, None, 4 917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_278 (BatchN (None, None, None, 4 1344        conv2d_278[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_278 (Activation)     (None, None, None, 4 0           batch_normalization_278[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_275 (Conv2D)             (None, None, None, 3 786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_279 (Conv2D)             (None, None, None, 3 1548288     activation_278[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_275 (BatchN (None, None, None, 3 1152        conv2d_275[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_279 (BatchN (None, None, None, 3 1152        conv2d_279[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_275 (Activation)     (None, None, None, 3 0           batch_normalization_275[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_279 (Activation)     (None, None, None, 3 0           batch_normalization_279[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_276 (Conv2D)             (None, None, None, 3 442368      activation_275[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_277 (Conv2D)             (None, None, None, 3 442368      activation_275[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_280 (Conv2D)             (None, None, None, 3 442368      activation_279[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_281 (Conv2D)             (None, None, None, 3 442368      activation_279[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_27 (AveragePo (None, None, None, 2 0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_274 (Conv2D)             (None, None, None, 3 655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_276 (BatchN (None, None, None, 3 1152        conv2d_276[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_277 (BatchN (None, None, None, 3 1152        conv2d_277[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_280 (BatchN (None, None, None, 3 1152        conv2d_280[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_281 (BatchN (None, None, None, 3 1152        conv2d_281[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_282 (Conv2D)             (None, None, None, 1 393216      average_pooling2d_27[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_274 (BatchN (None, None, None, 3 960         conv2d_274[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_276 (Activation)     (None, None, None, 3 0           batch_normalization_276[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_277 (Activation)     (None, None, None, 3 0           batch_normalization_277[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_280 (Activation)     (None, None, None, 3 0           batch_normalization_280[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_281 (Activation)     (None, None, None, 3 0           batch_normalization_281[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_282 (BatchN (None, None, None, 1 576         conv2d_282[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_274 (Activation)     (None, None, None, 3 0           batch_normalization_274[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, None, None, 7 0           activation_276[0][0]             \n",
      "                                                                 activation_277[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, None, None, 7 0           activation_280[0][0]             \n",
      "                                                                 activation_281[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_282 (Activation)     (None, None, None, 1 0           batch_normalization_282[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, None, None, 2 0           activation_274[0][0]             \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_6[0][0]              \n",
      "                                                                 activation_282[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 21,768,352\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set up the Dream configuration\n",
    "model.summary()\n",
    "layer_contributions={\n",
    "    'mixed2':0.2,\n",
    "    'mixed3':3.,\n",
    "    'mixed4':2.,\n",
    "    'mixed5':1.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss to be maximized\n",
    "layer_dict=dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "loss=K.variable(0.)\n",
    "for layer_name in layer_contributions:\n",
    "    # add the L2 norm of the features of a layer to the loss\n",
    "    coeff=layer_contributions[layer_name]\n",
    "    activation=layer_dict[layer_name].output\n",
    "    \n",
    "    # avoid border artfacts by only involving non-border pixels in the loss\n",
    "    scaling=K.prod(K.cast(K.shape(activation),'float32'))\n",
    "    loss += coeff*K.sum(K.square(activation[:,2:-2,2:-2,:]))/scaling\n",
    "    \n",
    "dream=model.input\n",
    "grads=K.gradients(loss,dream)[0]\n",
    "grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)\n",
    "\n",
    "# set up function to retrieve the value of the loss and gradients given an input image\n",
    "outputs=[loss,grads]\n",
    "fetch_loss_and_grads=K.function([dream],outputs)\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "    outs=fetch_loss_and_grads([x])\n",
    "    loss_value=outs[0]\n",
    "    grad_values=outs[1]\n",
    "    return loss_value, grad_values\n",
    "\n",
    "def gradient_ascent(x, iterations, step, max_loss=None):\n",
    "    for i in range(iterations):\n",
    "        loss_value, grad_values=eval_loss_and_grads(x)\n",
    "        if max_loss is not None and loss_value>max_loss:\n",
    "            break\n",
    "        print('...Loss value at',i,':',loss_value)\n",
    "        x += step*grad_values\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def resize_img(img, size):\n",
    "    img=np.copy(img)\n",
    "    factors=(1, float(size[0])/img.shape[1], float(size[1])/img.shape[2], 1)\n",
    "    return scipy.ndimage.zoom(img, factors, order=1)\n",
    "\n",
    "def save_img(img, fname):\n",
    "    pil_img=deprocess_image(np.copy(img))\n",
    "    scipy.misc.imsave(fname, pil_img)\n",
    "    \n",
    "def preprocess_image(image_path):\n",
    "    img=image.load_img(image_path)\n",
    "    img=image.img_to_array(img)\n",
    "    img=np.expand_dims(img, axis=0)\n",
    "    img=inception_v3.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):\n",
    "    if K.image_data_format()=='channels_first':\n",
    "        x=x.reshape((3, x.shape[2], x.shape[3]))\n",
    "        x=x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x=x.reshape((x.shape[1], x.shape[2], 3))\n",
    "    x /= 2.\n",
    "    x += 0.5\n",
    "    x *= 255\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image shape (255, 383)\n",
      "...Loss value at 0 : 1.7892327\n",
      "...Loss value at 1 : 2.3307054\n",
      "...Loss value at 2 : 3.0866718\n",
      "...Loss value at 3 : 3.8079987\n",
      "...Loss value at 4 : 4.525241\n",
      "...Loss value at 5 : 5.2030315\n",
      "...Loss value at 6 : 5.832796\n",
      "...Loss value at 7 : 6.4598684\n",
      "...Loss value at 8 : 7.091593\n",
      "...Loss value at 9 : 7.613576\n",
      "...Loss value at 10 : 8.1455765\n",
      "...Loss value at 11 : 8.716384\n",
      "...Loss value at 12 : 9.212679\n",
      "...Loss value at 13 : 9.679715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image shape (357, 537)\n",
      "...Loss value at 0 : 2.9886038\n",
      "...Loss value at 1 : 4.307748\n",
      "...Loss value at 2 : 5.4929094\n",
      "...Loss value at 3 : 6.433222\n",
      "...Loss value at 4 : 7.2584724\n",
      "...Loss value at 5 : 8.04483\n",
      "...Loss value at 6 : 8.744614\n",
      "...Loss value at 7 : 9.4261875\n",
      "Processing image shape (500, 752)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\scipy\\ndimage\\interpolation.py:616: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Loss value at 0 : 2.854626\n",
      "...Loss value at 1 : 4.118005\n",
      "...Loss value at 2 : 5.2332892\n",
      "...Loss value at 3 : 6.186092\n",
      "...Loss value at 4 : 7.110281\n",
      "...Loss value at 5 : 7.9443383\n",
      "...Loss value at 6 : 8.736466\n",
      "...Loss value at 7 : 9.492383\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "step=0.01      # Gradient ascent step size\n",
    "num_octave=3   # Number of scales at which to run gradient ascent\n",
    "octave_scale=1.4  # Size ratio between scales\n",
    "iterations=20      # Number of steps per scale\n",
    "\n",
    "max_loss=10.\n",
    "\n",
    "base_image_path='/Users/think/Downloads/creative_commons_elephant.jpg'\n",
    "img=preprocess_image(base_image_path)\n",
    "\n",
    "original_shape=img.shape[1:3]\n",
    "successive_shapes=[original_shape]\n",
    "for i in range(1, num_octave):\n",
    "    shape=tuple([int(dim/(octave_scale**i)) for dim in original_shape])\n",
    "    successive_shapes.append(shape)\n",
    "    \n",
    "# reverse list of shapes, so that they are in increasing order\n",
    "successive_shapes=successive_shapes[::-1]\n",
    "\n",
    "# resize the Numpy array to our smallest scale\n",
    "original_img=np.copy(img)\n",
    "shrunk_original_img=resize_img(img, successive_shapes[0])\n",
    "\n",
    "for shape in successive_shapes:\n",
    "    print('Processing image shape',shape)\n",
    "    img=resize_img(img, shape)\n",
    "    img=gradient_ascent(img, iterations=iterations, step=step, max_loss=max_loss)\n",
    "    upscaled_shrunk_original_img=resize_img(shrunk_original_img, shape)\n",
    "    same_size_original=resize_img(original_img, shape)\n",
    "    lost_detail=same_size_original-upscaled_shrunk_original_img\n",
    "    \n",
    "    img += lost_detail\n",
    "    shrunk_original_img=resize_img(original_img, shape)\n",
    "    save_img(img, fname='dream_at_scale_'+str(shape)+'.png')\n",
    "    \n",
    "save_img(img, fname='final_dream.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
